{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a535b3f6",
   "metadata": {},
   "source": [
    "\n",
    "An important task in Data Science is privacy analysis. A good data scientist have to protect sensitive data. There are different methods that deals with this. We want to use local privacy to ensure that the data of the individuals are private. For this we use the laplace mechanism on continious variables,like income adn age. and we use ramdomized response for the cathegorical variables.\n",
    "\n",
    "Appliyng the laplace mechanims in a local private fashion is done by applying laplace noise to the continious variables.\n",
    "\n",
    "theory:\n",
    "$$ y_i=x_i+\\omega $$ where $\\omega$ is laplace distributed with parameter $\\lambda=\\frac{sensitivity}{\\epsilon}$ $\\frac{L}{\\lambda}$.This formula tells us that as we increase $\\lambda$ the variance of the laplace distribution increases.This means more privacy,but it also means that we are adding so much noise far away from the true data that the new data where the noise is added is useless. The process of adding laplace noise to the data is one of the themes in differential privacy.Looking more closely at the formula one could also understand the meaning of the term epsilon-privacy. Increasing the variance in the laplace distribution,adding noise that is further and further away from the true answer,is the same thing as decreasing epsilon. The larger epsilon is the more information you loose every time somebody is investigating your data.By investigating i mean appliyng a function to you your data in order to extract useful information about the data.Smaller epsilon means adding noise that doesnt change the true data. This relation exibits a tradeoff relationship. We want privacy but we also want to have a dataset that is useful.\n",
    " \n",
    " Now to find the optimal value of lambda is not only dependent on epsilon it is also dependent on the sensitivity of your investigate-function. \n",
    " \n",
    " Defenition of sensitivity:\n",
    " \n",
    " $$ L(u) = \\sup_{xNx'} |f(..,x) - f(...,x')|<L|x-x'|$$\n",
    " \n",
    " One has to define what a neighbor is in the data set(|x-x'|).L is the lipsitz constant of function f must be estimated by taking the derivative of the function,setting it equal to zero to find the max of the function. This is important to do to find the optimal $lambda$ value for the spesific problem. \n",
    " \n",
    " \n",
    "On the qualititive variables we will apply randomize response on some of the variables. Some of the comorbidities,gender,some of the symptoms after the treatment that are less prevelent,beacause the less prevelent the symptom is the easier it is to identify poeple with those symptoms. \n",
    "\n",
    "randomized response theory:\n",
    "The randomized response can be explained by a simple example.In the beginning you flip a coin. If the coin comes up tail you say yes to a yes and no question truthfully.If it comes up tail you flip a new coin if it turns up head answer yes and tail answer no. A coin has probability (theta)1/2 for turning up head and (1-theta) 1/2 for tail. This means that 75% of the time the answer will be yes. 50% of the observations are never changed. The other 50% get changed depending on what the true answer is.\n",
    "\n",
    "We can calculate the expected value of p in order to get an estimate of the true thruth in the population.\n",
    "\n",
    "$$E[p]=theta*\\frac{1}{2}+q*\\frac{1}{2}$$\n",
    "\n",
    "p is the observed rate of positive responses in a sample. q is the true positive reponse in the population.\n",
    "\n",
    "$$q=2*E[p]-\\frac{1}{2}$$.\n",
    "\n",
    "The problem with this approach is that we are throwing away half of the data when estimating q.\n",
    "\"if we repeated the experiment with a coin that came heads at a rate ϵ,\n",
    "then our error bounds would scale as O(1/√ϵn) for n data points\"\n",
    "This means that when n is small,we have a lot of uncertainty on where the $\\epsilon- D$ private observations ends up. So we get a dataset,that cant be used.Any estimate we make using this data set will have much less correlation between the data and the response variable.\n",
    "solving for q we get an estimate of the true truth.\n",
    "\n",
    "\n",
    "We are also going to use the exponential mechanism to add noise to the policy.\n",
    "\n",
    "\n",
    "Utility:\n",
    "\n",
    "\n",
    "\n",
    "One can show that in randomized response mechanism $\\epsilon$ ends up bieng $ln(\\frac{1-p}{p})$\n",
    "The larger $\\epsilon$ less privacy.The larger the epsilon the more similar the true data\n",
    "\n",
    "\n",
    " \n",
    "Explain how you would protect the data of the people in the training set. In particular, given that your policy and model are obtained from some 'training' data set, how would you guarantee that release, or use, of the policy and model does not leak private information about the individuals?\n",
    "\n",
    "Here we would add laplace noise to the data,with a fixed sensitivity=1,and vary epsilon.Creating a very similar data set that is almost indistinguishable from the true data.\n",
    "Some problems when n is small,but in this case n is big\n",
    "\n",
    "Explain how would protect the data of the people that obtain treatment. When you apply the policy or model to decide what treatment to give, this decision can be assumed to be publicly available. How would you then ensure that the private information of the treated individual is not leaked?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    " \n",
    " \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In this section, we will introduce two different types of differential privacy techniques such as Randomized Response and Exponential Mechanism. The topics we will concentrate on is how can we make sure that the existence of a specific database does not raise any privacy concerns, and if such database was hidden but the analysis is public, how will that affect privacy?\n",
    "\n",
    "For the rest of the report, we will mainly concentrate on the treatment data set. We will later construct a new policy for treatment decisions.\n",
    "\n",
    "\\section*{The randomized response}\n",
    "We can protect the treatment data by using the randomized response. For treatment data with n samples ${x_1 ... x_n}$, we transform these into ${a_1...a_n}$ by following local privacy model:\n",
    "\\begin{lstlisting}[language=Python]\n",
    "for each x_i:\n",
    "    flip a coin\n",
    "    if coin == head:\n",
    "        a_i = x_i\n",
    "    else:\n",
    "        flip a coin\n",
    "        if coin == head:\n",
    "            a_i = x_i \n",
    "        else:\n",
    "            a_i != x_i \n",
    "\\end{lstlisting}\n",
    "In other words, the model is about flipping a coin. If the flip results in head then we send in the true sample, and if it results in tail the model will flip a coin again. This time if it is a head we will annotate $a_i = x_i$ and $a_i \\neq x_i$ otherwise. In this way, we make sure that the differential privacy criterion is satisfied. This means that we do not risk leaking private information about the individuals by releasing an obtained privacy and model which is trained on a training data from a randomized response mechanism. Because the responses are totally random and the rate of true data is \n",
    "$$ q =  2 E_p - \\frac{1}{2}$$\n",
    "where $E_p$ is the expectation of rate of positive responses [1]. This leads to 0-Differential privacy [2], which will result in better privacy but less accurate response [3].\n",
    "\n",
    "\\section*{The exponential mechanism}\n",
    "The exponential mechanism (EM) is one of the most general privacy differential mechanisms. The ME mechanism selects and outputs an element r from a range of elements $\\mathfrak{R}$ with probability proportional to \n",
    "$$ \\exp(\\frac{\\epsilon u(x,v)}{L(u)}) $$\n",
    "[4] where $u(x,v)$ is a utility function which maps subset of the data into utility scores and the $L(u)$ is the sensitivity of a query, i.e the maximum change between subset x and y. The best part of EM is that the privacy loss is approximately [4]\n",
    "$$ \\ln \\Bigg( \\frac{\\exp(\\frac{\\epsilon u(x,v)}{L(u)}) }{\\exp(\\frac{\\epsilon u(y,v)}{L(u)}) } \\Bigg) \\leq \\epsilon$$\n",
    "In other words, the loss of privacy is less or equal to the $\\epsilon$ and it preserves the $\\epsilon$-DP. This mechanism is much better than the others since it releases less information about individuals, and more about the elements with most noise. The decisions of what treatment to give to individuals can be assumed to be publicly available, and we ensure that none of individual information is leaked by using the EM. \n",
    "\n",
    "\\section*{Implementation of the exponential mechanism}\n",
    "The code below shows the implementation of the of the exponential mechanism. A running example of this code is available at\n",
    "\\href{https://github.com/rohullaa/fairness_and_privacy}{ \\textcolor{blue}{GitHub} }.\n",
    "\\newpage\n",
    "\n",
    "\\begin{lstlisting}\n",
    "class PrivacyPolicy:\n",
    "    def __init__(self, features, actions, outcome):\n",
    "        self.features = features\n",
    "        self.actions = actions\n",
    "        self.outcome = outcome\n",
    "    def exponential_mechanism(self, epsilon):\n",
    "        \"\"\"\n",
    "        Given a set of actions and a utility function, this function returns the noisy 'best' action.\n",
    "        Since our utility function at the moment does not depend on actions, only outcome, the results are expected to be random.  \n",
    "        \"\"\"\n",
    "        best_actions = []\n",
    "        for i in range(self.features.shape[0]):\n",
    "            utility = np.array([self.get_utility(self.features.iloc[i,:], action, self.outcome.iloc[i,:]) for action in self.actions.iloc[i,:]])\n",
    "            policy_probs = np.exp(epsilon*utility/2*self.sensitivity)\n",
    "            policy_probs = policy_probs/np.linalg.norm(policy_probs, ord=1)\n",
    "            best_actions.append(np.random.choice(self.actions.columns, 1, p=policy_probs.ravel())[0])\n",
    "        return best_actions\n",
    "    def get_utility(self, features, action, outcome):\n",
    "        utility = 0\n",
    "        utility -= 0.2 * sum(outcome[['Covid-Positive']])\n",
    "        utility -= 0.1 * sum(outcome[['Taste']])\n",
    "        utility -= 0.1 * sum(outcome[['Fever']])\n",
    "        utility -= 0.1 * sum(outcome[['Headache']])\n",
    "        utility -= 0.5 * sum(outcome[['Pneumonia']])\n",
    "        utility -= 0.2 * sum(outcome[['Stomach']])\n",
    "        utility -= 0.5 * sum(outcome[['Myocarditis']])\n",
    "        utility -= 1.0 * sum(outcome[['Blood-Clots']])\n",
    "        utility -= 100.0 * sum(outcome[['Death']])\n",
    "        self.sensitivity = 100\n",
    "        return utility\n",
    "\\end{lstlisting}\n",
    "\n",
    "\n",
    "\n",
    "\\section*{Utility function}\n",
    "We first define some preferences A and B, and prefer A to B if  $U(A)>U(B)$. Our preference should not be cyclical, e.g $A \\sim B \\sim C \\sim A$. The set of all preferences defined given a theme, is called reward (r).\n",
    "The properties that the elements in the set should have is: \n",
    "\n",
    "\\begin{itemize}\n",
    "    \\item For every element A and B in the set of rewards either you have A preferred to B, B preferred to A or A indifferent to B.\n",
    "    \\item If A is preferred to B and B is preferred to C then A is preferred to C.\n",
    "\\end{itemize}\n",
    "\n",
    "The utility depends on some set of actions when applied to $x$ (observations) gives a real output. The utility $U(a,y)$, $a$ depends on $x$. Since the action is finite and the outcome is finite, then utility is a discrete function. The action set is defined as follows. \n",
    "\n",
    "$$ A= \\{a_0,a_1,a_2,a_3 \\} $$\n",
    "where $a$'s are \n",
    "\\begin{itemize}\n",
    "    \\item $a_0$ is the action of not treating an individual with any treatment\n",
    "    \\item $a_1$ is the action of treating an individual with treatment 1\n",
    "    \\item $a_2$ is the action of treating an individual with treatment 2\n",
    "    \\item $a_3$ is the action of treating an individual with treatment 2 and treatment1\n",
    "\\end{itemize}\n",
    "\n",
    "The reward for each person is defined as a table. The utility becomes \n",
    "$$\\sum_i r_i $$ \n",
    "where $r_i$ is the reward table. For each row we choose the action, given features and outcome, that maximizes the reward. We sum all the rewards for all observations to get the utility. The reward will be different depending on the state of the individual in the row. \\\\\n",
    "\n",
    "\\noindent Every symptom has a negative contribution to the reward depending on the severity of symptom. Choosing not to treat a patient with lot of symptoms will have a large negative reward. In other words, choosing to treat this patient will be the preferred action. In contrast, a patient with close to no symptoms will not benefit from a treatment and the preferred action should be not to treat, i.e $a_0$. We assume that the policy is not restricted, i.e there is a treatment for everyone who needs it. If the policy was restricted, we would have to choose which patients would need the treatment the most. For example, consider the following patient\n",
    "\n",
    "\\begin{center}\n",
    "\\begin{tabular}{ c c}\n",
    "Age & 60  \\\\ \n",
    "No-Taste/Smell  & yes  \\\\ \n",
    "Fever  & yes \\\\ \n",
    "Headache  & yes \\\\ \n",
    "Pneumonia  & yes \\\\ \n",
    "Stomach  & yes \\\\ \n",
    "Myocarditis  & yes \\\\ \n",
    "Blood-Clots & yes \\\\ \n",
    "Asthma & yes \\\\ \n",
    "Obesity & yes \\\\ \n",
    "Smoking &  yes \\\\ \n",
    "Diabetes &  yes \\\\ \n",
    "Heart disease & yes \\\\ \n",
    "Hypertension & yes \\\\ \n",
    "\n",
    "\\end{tabular}\n",
    "\\end{center}\n",
    "\n",
    "\\noindent For the example above the policy could be something like\n",
    "$$ \\pi(a|x) = \\{ a_0: 0.0, a_1: 0.2 , a_2:  0.2  a_3: 0.6 \\} $$\n",
    "\n",
    "\\section*{Sensitivity of the utility function}\n",
    "In order to estimate the amount of loss in utility as we change the privacy guarantee, we compute the sensitivity of the utility. It is defined as \n",
    "$$ L(u) = \\sup_{xNx'} |u(a,x) - u(a,x')|$$\n",
    "\n",
    "\\section*{Bibliografi}\n",
    "\\noindent [1] Chapter 3.4 in Machine learning in science and society,\\newline (https://github.com/olethrosdc/ml-society-science/blob/master/notes.pdf).\\newline\n",
    "\n",
    "\\noindent [2]  Machine learning in science and society - Remark 3.4.2,\\newline (https://github.com/olethrosdc/ml-society-science/blob/master/notes.pdf).\\newline\n",
    "\n",
    "\\noindent [3] Shaistha Fathima - Sep 15h 2020,\\newline (https://medium.com/@shaistha24/differential-privacy-definition-bbd638106242). \\newline\n",
    "\n",
    "\\noindent [4] C. Dwork, A. Roth - The Algorithmic Foundations of Differential Privacy - chapter 3.4,\\newline (https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf). \\newline\n",
    "\n",
    "\\end{document}\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
